{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6jqeXvJfpGyP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gh9BhJEkuA9I"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60784,"status":"ok","timestamp":1661141338630,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"Nmb8APSipLAF","outputId":"b6e368e8-7f38-44dc-c304-4a480b695b36"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers[ja]\n","  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (1.21.6)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 10.8 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,\u003c0.13,\u003e=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 40.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2022.6.2)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (4.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (2.23.0)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[ja]) (21.3)\n","Collecting ipadic\u003c2.0,\u003e=1.0.0\n","  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n","\u001b[K     |████████████████████████████████| 13.4 MB 26.9 MB/s \n","\u001b[?25hCollecting fugashi\u003e=1.0\n","  Downloading fugashi-1.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (568 kB)\n","\u001b[K     |████████████████████████████████| 568 kB 44.0 MB/s \n","\u001b[?25hCollecting unidic\u003e=1.0.2\n","  Downloading unidic-1.1.0.tar.gz (7.7 kB)\n","Collecting unidic-lite\u003e=1.0.7\n","  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n","\u001b[K     |████████████████████████████████| 47.4 MB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers[ja]) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers[ja]) (3.0.9)\n","Requirement already satisfied: wasabi\u003c1.0.0,\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from unidic\u003e=1.0.2-\u003etransformers[ja]) (0.10.1)\n","Collecting plac\u003c2.0.0,\u003e=1.1.3\n","  Downloading plac-1.3.5-py2.py3-none-any.whl (22 kB)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers[ja]) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers[ja]) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers[ja]) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers[ja]) (2022.6.15)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers[ja]) (3.8.1)\n","Building wheels for collected packages: ipadic, unidic, unidic-lite\n","  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=600049b8a0e103018c3f54e928ee60440ace764012c72bd62489d78ea656240b\n","  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n","  Building wheel for unidic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic: filename=unidic-1.1.0-py3-none-any.whl size=7426 sha256=5374c75722eba0de5ac08fb4dac330059c572cc02ca777ed40e22d7dad70389a\n","  Stored in directory: /root/.cache/pip/wheels/ce/4d/f1/170bb74b559ca338113c0315c9805e16dfd0a12411ec6b1122\n","  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658836 sha256=b711ff697d732396df9a3f270f9a178db185c89d61dea7d4a7c44857cfc6202a\n","  Stored in directory: /root/.cache/pip/wheels/de/69/b1/112140b599f2b13f609d485a99e357ba68df194d2079c5b1a2\n","Successfully built ipadic unidic unidic-lite\n","Installing collected packages: tokenizers, plac, huggingface-hub, unidic-lite, unidic, transformers, ipadic, fugashi\n","Successfully installed fugashi-1.1.2 huggingface-hub-0.8.1 ipadic-1.0.0 plac-1.3.5 tokenizers-0.12.1 transformers-4.21.1 unidic-1.1.0 unidic-lite-1.0.8\n","\u001b[K     |████████████████████████████████| 1.3 MB 4.4 MB/s \n","\u001b[?25h"]}],"source":["!pip install transformers[ja]\n","!pip install --quiet sentencepiece"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45207,"status":"ok","timestamp":1661141383819,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"N68LQgs7pYUK","outputId":"827d5254-45b0-4e9c-b68f-fe55bc3e3069"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K     |████████████████████████████████| 4.0 MB 4.3 MB/s \n","\u001b[K     |████████████████████████████████| 880 kB 58.2 MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 4.1 MB 3.2 MB/s \n","\u001b[?25h  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install --quiet transformers==4.18.0\n","!pip install --quiet tokenizers==0.12.1\n","!pip install --quiet sentencepiece\n","!pip install --quiet japanize-matplotlib\n","!pip install transformers fugashi ipadic \u003e\u003e /dev/null"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6143,"status":"ok","timestamp":1661141389897,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"RyDRa3HLpeAr","outputId":"ac8680db-a6b4-4dbb-c35f-d02625b1427a"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from PIL import Image\n","import japanize_matplotlib\n","\n","import torch\n","import transformers\n","from transformers import BertTokenizer, BertJapaneseTokenizer\n","\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import log_loss\n","from sklearn.model_selection import StratifiedKFold, KFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","import sys\n","import os\n","import re\n","import random\n","\n","from time import time\n","from tqdm import tqdm\n","\n","from contextlib import contextmanager\n","import lightgbm as lgb\n","\n","import re\n","import requests\n","import unicodedata\n","import nltk\n","from nltk.corpus import wordnet\n","from bs4 import BeautifulSoup\n","nltk.download(['wordnet', 'stopwords', 'punkt'])"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1661141389898,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"3Makfqiwpy6v"},"outputs":[],"source":["def seed_everything(seed=1234):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","seed_everything(42)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1661141389899,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"dcyli96Vpzl-"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1661141389901,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"6zXpx1EOqGF5"},"outputs":[],"source":["INPUT = \"/content/drive/MyDrive/bokeAI/\" # 所望のディレクトリに変更してください。\n","train_image_path = \"/content/drive/MyDrive/bokeAI/train\"\n","test_image_path = \"/content/drive/MyDrive/bokeAI/test\""]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"elapsed":810,"status":"error","timestamp":1661141390695,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"_a994dxnGM9i","outputId":"b0366ce3-13e3-49a1-e90c-6a855568f9a8"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-6-c7f05beae292\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msubmission_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sample_submission.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--\u003e 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/bokeAI/train.csv'"]}],"source":["train_df = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\n","test_df = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\n","submission_df = pd.read_csv(os.path.join(INPUT, \"sample_submission.csv\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":23,"status":"aborted","timestamp":1661141390696,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"NDS_DtR_GM6_"},"outputs":[],"source":["print(f\"train_data: {train_df.shape}\")\n","display(train_df.head())\n","\n","print(f\"test_data: {test_df.shape}\")\n","display(test_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":55,"status":"aborted","timestamp":1661141390728,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"LSeclAVtGM4Z"},"outputs":[],"source":["train_df[train_df['is_laugh']==1]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":55,"status":"aborted","timestamp":1661141390729,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"ej8Ysd58GM2P"},"outputs":[],"source":["# 目的変数の分布を確認する\n","sns.countplot(x=\"is_laugh\", data=train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":56,"status":"aborted","timestamp":1661141390730,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"ecuuuYQAGM0C"},"outputs":[],"source":["# 画像の重複があるか確認する\n","print(train_df[\"odai_photo_file_name\"].duplicated().sum())\n","print(test_df[\"odai_photo_file_name\"].duplicated().sum())"]},{"cell_type":"markdown","metadata":{"id":"VGUJXtkqJH7Z"},"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":56,"status":"aborted","timestamp":1661141390730,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"WkCLGNwEGMxQ"},"outputs":[],"source":["# 文書の長さとflagの関係を確認する\n","train_df[\"text_len\"] = train_df[\"text\"].str.len()\n","test_df[\"text_len\"] = test_df[\"text\"].str.len()\n","\n","sns.boxplot(x=\"is_laugh\", y=\"text_len\", data=train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":56,"status":"aborted","timestamp":1661141390731,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"bvqX5Ov8GMu2"},"outputs":[],"source":["# データ内にあるボケてを確認してみる\n","\n","fig = plt.figure(figsize=(15,7))\n","\n","data1 = train_df[train_df[\"is_laugh\"]==1]\n","image_path1 = train_image_path +'/'+data1.iloc[0][\"odai_photo_file_name\"]\n","img1 = img = Image.open(image_path1)\n","\n","ax1 = fig.add_subplot(1,2,1)\n","ax1.imshow(img1)\n","plt.title(str(data1.iloc[0][\"is_laugh\"])+ \": \"+ data1.iloc[0][\"text\"])\n","\n","data2 = train_df[train_df[\"is_laugh\"]==0]\n","image_path2 = train_image_path +'/'+data2.iloc[0][\"odai_photo_file_name\"]\n","img2 = img = Image.open(image_path2)\n","\n","ax2 = fig.add_subplot(1,2,2)\n","ax2.imshow(img2)\n","plt.title(str(data2.iloc[0][\"is_laugh\"])+ \": \"+ data2.iloc[0][\"text\"])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0pCIx-qnMnR3"},"source":["個人的な主観も含まれてしまいますが、以下のようなことがわかってきました。  \n","- ファイル名で確認すると、画像の重複はtrainとtestではない。\n","- ボケての文章の長さを確認すると、面白いボケての方が文章が長い傾向\n","- 画像とボケての文章を見てみると、確かに１と０で差がありそう（個人的に０の方はどのようにボケているのかが少しわかりにくい感じがあります）\n","\n","今回は単純に文章の長さのみを確認しましたが、他にも文章自体について注目していくと、面白さに関する知見というのが見えてくるかも知れません。\n"]},{"cell_type":"markdown","metadata":{"id":"bHqtzCblMq0A"},"source":["# Create Image Features\n","\n","ボケてというものは、画像と文章の組み合わせで面白さを表現しているので、以下にして画像のデータと文章のデータをモデルに学習させるかがポイントになってくるかと思います。\n","\n","画像のデータを特徴量として用いるために、今回はDenseNet121の学習済みモデルを用います。"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57,"status":"aborted","timestamp":1661141390732,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"T1mcy7UVGMqx"},"outputs":[],"source":["import cv2\n","from keras.models import Model\n","from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n","import keras.backend as K\n","from tqdm import tqdm, tqdm_notebook\n","from keras.applications.densenet import preprocess_input, DenseNet121"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57,"status":"aborted","timestamp":1661141390732,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"biyZizA9MvUO"},"outputs":[],"source":["class CFG:\n","    img_size = 224\n","    batch_size = 17"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":58,"status":"aborted","timestamp":1661141390734,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"PIQ7WInQM1xo"},"outputs":[],"source":["def resize_to_square(im):\n","    old_size = im.shape[:2] \n","    ratio = float(CFG.img_size)/max(old_size)\n","    new_size = tuple([int(x*ratio) for x in old_size])\n","    # 画像サイズを224×224に変更します\n","    im = cv2.resize(im, (new_size[1], new_size[0]))\n","    delta_w = CFG.img_size - new_size[1]\n","    delta_h = CFG.img_size - new_size[0]\n","    top, bottom = delta_h//2, delta_h-(delta_h//2)\n","    left, right = delta_w//2, delta_w-(delta_w//2)\n","    color = [0, 0, 0]\n","    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n","    return new_im\n","\n","\n","def load_image(ids, is_train=True):\n","  if is_train:\n","    image = cv2.imread(train_image_path+'/'+ids)\n","  else:\n","    image = cv2.imread(test_image_path+'/'+ids)\n","  new_image = resize_to_square(image)\n","  new_image = preprocess_input(new_image)\n","  return new_image"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":59,"status":"aborted","timestamp":1661141390735,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"Gs-pRc42Eqfb"},"outputs":[],"source":["inp=Input((224,224,3))\n","backbone=DenseNet121(input_tensor=inp,include_top=False)\n","x=backbone.output\n","x=GlobalAveragePooling2D()(x)\n","x=Lambda(lambda x:K.expand_dims(x,axis=-1))(x)\n","x=AveragePooling1D(4)(x)\n","out=Lambda(lambda x: x[:,:,0])(x)\n","\n","m=Model(inp,out)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":59,"status":"aborted","timestamp":1661141390735,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"JyaxWe2HNBBm"},"outputs":[],"source":["image_df_train = train_df[[\"id\", \"odai_photo_file_name\"]].copy()\n","image_df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":62,"status":"aborted","timestamp":1661141390738,"user":{"displayName":"たかいです","userId":"08363226705441905685"},"user_tz":-540},"id":"ilk_BhjXNFZC"},"outputs":[],"source":["image_ids = image_df_train[\"odai_photo_file_name\"].values\n","n_batches = len(image_ids) // CFG.batch_size + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fOOcJvTNXH8","outputId":"eb631dc3-ad43-41d0-b792-b26dea17fd24"},"outputs":[{"name":"stderr","output_type":"stream","text":["  7%|▋         | 103/1469 [27:11\u003c5:53:20, 15.52s/it]"]}],"source":["features = {}\n","for b in tqdm(range(n_batches)):\n","    start = b*CFG.batch_size\n","    end = (b+1)*CFG.batch_size\n","    batch_ids = image_ids[start:end]\n","    batch_images = np.zeros((len(batch_ids),CFG.img_size,CFG.img_size,3))\n","    for i,image_id in enumerate(batch_ids):\n","        try:\n","            batch_images[i] = load_image(image_id)\n","        except:\n","          print(\"Error\")\n","    batch_preds = m.predict(batch_images)\n","    for i,image_id in enumerate(batch_ids):\n","        features[image_id] = batch_preds[i]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"StK5c808NZAT"},"outputs":[],"source":["image_feature = pd.DataFrame.from_dict(features, orient='index').add_prefix(\"DenseNet121_\").reset_index()\n","image_feature.rename(columns={\"index\":\"odai_photo_file_name\"}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dLrYmCt_NvOM"},"outputs":[],"source":["image_feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0M5H3ycUNAZ"},"outputs":[],"source":["# trainのデータに結合します。\n","train_df = pd.merge(train_df, image_feature, on=\"odai_photo_file_name\", how=\"left\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8clroNoYcFZ"},"outputs":[],"source":["train_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XW-Ecx2bYdzm"},"outputs":[],"source":["# testデータでも同様なことを行って行きます\n","image_df_test = test_df[[\"id\", \"odai_photo_file_name\"]].copy()\n","\n","image_ids = image_df_test[\"odai_photo_file_name\"].values\n","n_batches = len(image_ids) // CFG.batch_size + 1\n","\n","\n","features = {}\n","for b in tqdm(range(n_batches)):\n","    start = b*CFG.batch_size\n","    end = (b+1)*CFG.batch_size\n","    batch_ids = image_ids[start:end]\n","    batch_images = np.zeros((len(batch_ids),CFG.img_size,CFG.img_size,3))\n","    for i,image_id in enumerate(batch_ids):\n","        try:\n","            batch_images[i] = load_image(image_id, is_train=False)\n","        except:\n","          print(\"Error\")\n","    batch_preds = m.predict(batch_images)\n","    for i,image_id in enumerate(batch_ids):\n","        features[image_id] = batch_preds[i]\n","\n","image_feature = pd.DataFrame.from_dict(features, orient='index').add_prefix(\"DenseNet121_\").reset_index()\n","image_feature.rename(columns={\"index\":\"odai_photo_file_name\"}, inplace=True)\n","\n","test_df = pd.merge(test_df, image_feature, on=\"odai_photo_file_name\", how=\"left\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GFM3TcGZYh5N"},"outputs":[],"source":["test_df.shape"]},{"cell_type":"markdown","metadata":{"id":"5smcNytoYlgW"},"source":["# Create Text Features\n","\n","続いてボケての文章について、BERTモデルを用いて特徴量化していきます。\n","特徴量化については、以下のディスカッションを参考にさせていただきます。  \n","[japanese-roberta-baseでテキストデータをembeddingする(小説家になろう ブクマ数予測 \\~”伸びる”タイトルとは？\\~ より)](https://www.nishika.com/competitions/21/topics/163)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWFBOTouYjp1"},"outputs":[],"source":["def clean_text(text):\n","    replaced_text = text.lower()\n","    replaced_text = re.sub(r'[【】]', ' ', replaced_text)       # 【】の除去\n","    replaced_text = re.sub(r'[（）()]', ' ', replaced_text)     # （）の除去\n","    replaced_text = re.sub(r'[［］\\[\\]]', ' ', replaced_text)   # ［］の除去\n","    replaced_text = re.sub(r'[『』]', ' ', replaced_text)   # 『』の除去\n","    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n","    replaced_text = re.sub(r'https?:\\/\\/.*?[\\r\\n ]', '', replaced_text)  # URLの除去\n","    replaced_text = re.sub(r'　', ' ', replaced_text)  # 全角空白の除去\n","    replaced_text = re.sub(r' ', '', replaced_text)  # 空白の除去\n","    return replaced_text\n","\n","\n","def clean_html_tags(html_text):\n","    soup = BeautifulSoup(html_text, 'html.parser')\n","    cleaned_text = soup.get_text()\n","    cleaned_text = ''.join(cleaned_text.splitlines())\n","    return cleaned_text\n","\n","\n","def clean_html_and_js_tags(html_text):\n","    soup = BeautifulSoup(html_text, 'html.parser')\n","    [x.extract() for x in soup.findAll(['script', 'style'])]\n","    cleaned_text = soup.get_text()\n","    cleaned_text = ''.join(cleaned_text.splitlines())\n","    return cleaned_text\n","\n","\n","def clean_url(html_text):\n","    cleaned_text = re.sub(r'http\\S+', '', html_text)\n","    return cleaned_text\n","\n","\n","def normalize(text):\n","    normalized_text = normalize_unicode(text)\n","    normalized_text = normalize_number(normalized_text)\n","    normalized_text = lower_text(normalized_text)\n","    return normalized_text\n","\n","\n","def lower_text(text):\n","    return text.lower()\n","\n","\n","def normalize_unicode(text, form='NFKC'):\n","    normalized_text = unicodedata.normalize(form, text)\n","    return normalized_text\n","\n","\n","def normalize_number(text):\n","    replaced_text = re.sub(r'\\d+', '0', text)\n","    return replaced_text\n","\n","\n","def text_cleaning(text):\n","    text = clean_text(text)\n","    text = clean_html_tags(text)\n","    text = clean_html_and_js_tags(text)\n","    text = clean_url(text)\n","    text = normalize(text)\n","    text = lower_text(text)\n","    text = normalize_unicode(text)\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXcDyWH0Yq1F"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPM5dbP4WvNVJrpqAMb8J3j","collapsed_sections":[],"name":"bokeAI.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}